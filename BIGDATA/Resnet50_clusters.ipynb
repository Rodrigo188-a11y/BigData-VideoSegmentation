{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import RANSACRegressor, LinearRegression\n",
    "import cv2\n",
    "import glob as glob\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "from keras.layers.core import Dropout\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.resnet import  ResNet50\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from numpy import savetxt\n",
    "from numpy import loadtxt\n",
    "import random\n",
    "from scipy import stats\n",
    "from matplotlib.gridspec import GridSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current Directory\n",
    "cur_dir = \"/Users/natanviana/Desktop/big_data\"\n",
    "\n",
    "# Initializing the Tranfer Learning Model Resnet50\n",
    "my_new_model = Sequential()\n",
    "my_new_model.add(ResNet50(include_top=False, pooling='avg', weights=\"imagenet\"))\n",
    "\n",
    "# Say not to train first layer (ResNet) model. It is already trained\n",
    "my_new_model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Function to save the output of images after pass inside a Resnet50 Model\n",
    "def extract_vector(path,resnet_feature_list):\n",
    "\n",
    "    im = cv2.imread(path)\n",
    "    img = preprocess_input(np.expand_dims(im.copy(), axis=0))\n",
    "    resnet_feature = my_new_model.predict(img)\n",
    "    resnet_feature_np = np.array(resnet_feature)\n",
    "    if(currentframe==0):\n",
    "        resnet_feature_list = resnet_feature_np\n",
    "    else:\n",
    "        resnet_feature_list =np.append(resnet_feature_list,resnet_feature_np,axis=0)\n",
    "\n",
    "    return np.array(resnet_feature_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the name of the video  and the extension\n",
    "nameVideo = \"girosmallveryslow2.mp4\"\n",
    "nameVideo = nameVideo.split(\".\")\n",
    "extension = \".\" + nameVideo[1]\n",
    "nameVideo = nameVideo[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the name of cluster that you want to cluster again \n",
    "cluster = \"cluster2\"\n",
    "path_to_cluster = \"/\"+ nameVideo +\"/clusterImages/\"+ cluster\n",
    "\n",
    "# Getting the Path to the frames inside this folder\n",
    "list = os.listdir(cur_dir + path_to_cluster) # variavel\n",
    "number_files = len(list)\n",
    "number_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the extract_vector for every frame of the Folder\n",
    "currentframe = 0\n",
    "feature_list = np.zeros(shape=(1, 2048))\n",
    "while(currentframe<number_files):\n",
    "    feature_list = extract_vector(cur_dir+path_to_cluster+\"/\"+list[currentframe],feature_list) # variavel\n",
    "    currentframe = currentframe+1\n",
    "    print(feature_list.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the matrix of features in a .csv file to not run the cells of reduction above\n",
    "name = cluster + \"_list.csv\"\n",
    "savetxt(name, feature_list, delimiter=',')   #variavel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the features in a .csv file to the feature_list\n",
    "feature_list = loadtxt(name, delimiter=',') #variavel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform into a Dataframe\n",
    "feature_list = pd.DataFrame(feature_list)\n",
    "feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a reduction of dimension PCA algorithm\n",
    "pca = PCA(0.90)\n",
    "features_pca = pca.fit_transform(feature_list\n",
    ")\n",
    "\n",
    "# explained percentage of importance\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of components of PCA \n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform into a Dataframe\n",
    "features_pca = pd.DataFrame(features_pca)\n",
    "features_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the stats of new matrix features\n",
    "features_pca.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the matrix of features \n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(features_pca)\n",
    "df_features_pca_scaled = scaler.transform(features_pca)\n",
    "df_features_pca_scaled = pd.DataFrame(df_features_pca_scaled)\n",
    "df_features_pca_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try or not to use the T-SNE algorithm to reduce the complexity of our features if - you doesnt use this cell use the next one.\n",
    "tsne = TSNE(n_components=2)\n",
    "features_tsne = tsne.fit_transform(df_features_pca_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only get the matrix of features after the PCA and scaled.\n",
    "features_tsne = df_features_pca_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the matrix of features again\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(features_tsne)\n",
    "df_features_tsne_scaled = scaler.transform(features_tsne)\n",
    "df_features_tsne_scaled = pd.DataFrame(df_features_tsne_scaled)\n",
    "df_features_tsne_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeastSquareSolution(X,Y):       # multivariate regression\n",
    "    transpose = X.T\n",
    "    product = np.dot(transpose,X)\n",
    "    inverse = np.linalg.pinv(product)\n",
    "    product2 = np.dot(inverse,X.T)\n",
    "    B = np.dot(Y,product2)\n",
    "    return B    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing 1  removing Possible Outliers with a Random Sub-base\n",
    "number_of_sub_space = 20\n",
    "df_sub_base = df_features_tsne_scaled.sample(n=number_of_sub_space)\n",
    "\n",
    "df_sub_base = np.array(df_sub_base,  dtype = float)\n",
    "features = np.array(df_features_tsne_scaled,dtype = float)\n",
    "\n",
    "\n",
    "Pi = np.dot(df_sub_base.T,df_sub_base)\n",
    "lengh = int(len(Pi))\n",
    "PiN = np.identity(lengh) - Pi\n",
    "fi = np.dot(Pi,features.T)\n",
    "fiN = np.dot(PiN,features.T)\n",
    "\n",
    "di = np.sqrt(sum(np.multiply(fi, fi)))\n",
    "dn = np.sqrt(sum(np.multiply(fiN, fiN)))\n",
    "df = np.sqrt(sum(np.multiply(features, features)))\n",
    "\n",
    "Ri = di / df [:,None]\n",
    "Ri\n",
    "\n",
    "Rin = dn / df [:,None]\n",
    "Rin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing our Data\n",
    "\n",
    "plt.scatter(df_features_tsne_scaled[0],df_features_tsne_scaled[1])\n",
    "plt.show()\n",
    "plt.hist(df_features_tsne_scaled[0], bins = 20, rwidth=0.8,density=True)\n",
    "rng = np.arange(df_features_tsne_scaled[0].min(),df_features_tsne_scaled[0].max(),0.1)\n",
    "plt.plot(rng, stats.norm.pdf(rng,df_features_tsne_scaled[0].mean(),df_features_tsne_scaled[0].std()))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Possible Outilers Testing 2\n",
    "list = pd.DataFrame(list)\n",
    "df_features_tsne_scaled[\"frames\"] = list[0]\n",
    "df2 = df_features_tsne_scaled\n",
    "for i in range(1,len(df2.columns-1)):\n",
    "    min_thresold, max_thresold = df_features_tsne_scaled[i].quantile([0.001,0.999])\n",
    "    df2 =df2 [(df_features_tsne_scaled[i]<min_thresold) & (df_features_tsne_scaled[i]>max_thresold)]\n",
    "df2\n",
    "\n",
    "df_frames = df2[\"frames\"]\n",
    "df_frames = pd.DataFrame(df_frames)\n",
    "df2 = df2.drop(\"frames\",axis=\"columns\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df2)\n",
    "df2 = scaler.transform(df2)\n",
    "df_features_tsne_scaled = pd.DataFrame(df2)\n",
    "df_features_tsne_scaled\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Elbow Method to know the K number of K-Means Custering\n",
    "k_range = range(1,15)\n",
    "sse = []\n",
    "\n",
    "for k in k_range:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(df_features_tsne_scaled)\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "sse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Elbow Graph\n",
    "plt.plot(k_range,sse)\n",
    "plt.xlabel(\"K number\")\n",
    "plt.ylabel(\"Sum of Square Errors\")\n",
    "plt.title(\"K Value for Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the K-Means CLustering\n",
    "list = pd.DataFrame(list)\n",
    "numero_clusters = 2\n",
    "df_features_use = df_features_tsne_scaled\n",
    "km = KMeans(n_clusters=numero_clusters)\n",
    "cluster_features_predict = km.fit_predict(df_features_use)\n",
    "df_features_use[\"cluster\"] = cluster_features_predict\n",
    "df_features_use[\"frames\"] = list[0]\n",
    "df_features_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the cluster 0 of features after K-Means Clustering\n",
    "df = df_features_use[df_features_use.cluster == 0 ]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of Cluster vs Frame to try understand our Cluster \n",
    "df_use = df_features_use\n",
    "\n",
    "df1 = df_use[df_use.cluster==0]\n",
    "df2 = df_use[df_use.cluster==1]\n",
    "df3 = df_use[df_use.cluster==2]\n",
    "\n",
    "\n",
    "plt.scatter(df1.frames,df1.cluster,color=\"red\")\n",
    "plt.scatter(df2.frames,df2.cluster,color=\"green\")\n",
    "plt.scatter(df3.frames,df3.cluster,color=\"blue\")\n",
    "\n",
    "plt.xlabel(\"frames\")\n",
    "plt.ylabel(\"cluster\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot of Cluster vs Frame to try understand our Cluster for the first 100 frames\n",
    "plt.scatter(df1[0],df1[1],color=\"red\")\n",
    "plt.scatter(df2[0],df2[1],color=\"green\")\n",
    "plt.scatter(df3[0],df3[1],color=\"blue\")\n",
    "\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Folder for each Cluster with the all frames of them \n",
    "\n",
    "cur_dir = \"/Users/natanviana/Desktop/big_data\"\n",
    "framesFolder = path_to_cluster +\"/\" # variavel\n",
    "\n",
    "for x in range(numero_clusters):\n",
    "    dirName2 = 'cluster%d' % x\n",
    "    os.mkdir( cur_dir + framesFolder + dirName2)\n",
    "    vector = df_features_use[df_features_use.cluster == x].frames\n",
    "    vector = pd.DataFrame(vector)\n",
    "    for y in vector.frames:\n",
    "        print(y)\n",
    "        img = Image.open(cur_dir + framesFolder +  y)\n",
    "        img.save(cur_dir + framesFolder  + dirName2 +\"/\"+ y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Folder for each Cluster with the all video Frames of them.\n",
    "\n",
    "cur_dir = \"/Users/natanviana/Desktop/big_data\"\n",
    "os.mkdir(cur_dir+ \"/\"+ nameVideo+\"/clusterVideos/\"+cluster)\n",
    "for i in range(numero_clusters):\n",
    "    os.chdir(cur_dir)\n",
    "    dirName = \"cluster%d\" % i\n",
    "    image_folder = '.'  # Use the folder\n",
    "    video_name = dirName + '.avi'\n",
    "    os.chdir(cur_dir + path_to_cluster +\"/\"+dirName)\n",
    "    images = [img for img in os.listdir(image_folder) if img.endswith(\".jpg\") or\n",
    "            img.endswith(\".jpeg\") or img.endswith(\"png\")]\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "\n",
    "    # Array images should only consider\n",
    "    # the image files ignoring others if any\n",
    "\n",
    "    frame = cv2.imread(os.path.join(image_folder, images[0]))\n",
    "\n",
    "    # setting the frame width, height width\n",
    "    # the width, height of first image\n",
    "    height, width, layers = frame.shape\n",
    "    \n",
    "    video = cv2.VideoWriter(cur_dir+ \"/\"+ nameVideo+\"/clusterVideos/\"+cluster + \"/\" + video_name, fourcc, 1, (width, height))\n",
    "\n",
    "    # Appending the images to the video one by one\n",
    "    count=0\n",
    "    for image in images:\n",
    "        count+=1\n",
    "        \n",
    "        video.write(cv2.imread(os.path.join(image_folder, image)))\n",
    "    print(count)\n",
    "    # Deallocating memories taken for window creation\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()  # releasing the video generated\n",
    "    print(\"FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View video that you choose by name of cluster , ex : cluster0.avi\n",
    "\n",
    "cur_dir = \"/Users/natanviana/Desktop/big_data\"\n",
    "os.chdir(cur_dir)\n",
    "# Create a VideoCapture object and read from input file\n",
    "import cv2\n",
    "import time\n",
    "videoName = input('What is the video you want to open?\\n')\n",
    "name = cur_dir + \"/\" +  nameVideo +\"/clusterVideos/\"+ cluster+ \"/\" + videoName\n",
    "cap = cv2.VideoCapture(name)\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if (cap.isOpened() == False):\n",
    "    print(\"Error opening video  file\")\n",
    "\n",
    "# Read until video is completed\n",
    "while (cap.isOpened()):\n",
    "    time.sleep(0.5)\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    print(ret)\n",
    "    if ret == True:\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Frame', frame)\n",
    "\n",
    "        # Press Q on keyboard to  exit\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Break the loop\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# When everything done, release\n",
    "# the video capture object\n",
    "cap.release()\n",
    "\n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Viewing 100 images per cluster\n",
    "\n",
    "for i in range(numero_clusters):\n",
    "\n",
    "    cluster_name = \"cluster%d\" % i\n",
    "    fig = plt.figure(figsize=(70, 70), dpi=100,)\n",
    "    gs = GridSpec(10, 10, wspace=0, hspace=0)\n",
    "    listx = []\n",
    "    for x in range(100):\n",
    "        listx.append(fig.add_subplot(gs[x]))\n",
    "    for ax in fig.get_axes():\n",
    "        ax.tick_params(bottom=False, labelbottom=False, left=False, labelleft=False)\n",
    "\n",
    "    \n",
    "    path_of_the_directory = cur_dir + \"/\"+ nameVideo + \"/clusterImages/\"+cluster+ \"/\" + cluster_name  # onde vai buscar imagens\n",
    "    number_images = 100  # define número de imagens a plotar\n",
    "    rows = 10\n",
    "    columns = 10\n",
    "\n",
    "    # faz lista com todas as imagens\n",
    "    filename = os.listdir(path_of_the_directory)\n",
    "    for x in range(number_images):  # vai retirar 100 imagens\n",
    "        chooseImg = random.choice(filename)  # chooses image at random\n",
    "        filename.remove(chooseImg)  # removes this image from the list\n",
    "        # reading images\n",
    "        img = Image.open(path_of_the_directory + '/' + chooseImg)\n",
    "        #  width, height = img.size\n",
    "        listx[x].imshow(img, aspect='auto')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60bbdff2961aca4aff0f1f8ecff43981e9d174e6a32ce20cdbf019496f08ed1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
